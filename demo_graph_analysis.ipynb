{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f8b707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Graph clustering and graph classification\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import scipy as sp\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch_geometric\n",
    "from torch_geometric.datasets import TUDataset\n",
    "from scipy.sparse import csr_matrix\n",
    "from typing import Dict, Tuple\n",
    "\n",
    "from sklearn.cluster import SpectralClustering\n",
    "from sklearn import metrics\n",
    "from sklearn import svm\n",
    "\n",
    "from methods.gromov_funcs import define_loss_function, gw_distance, spar_gw\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab53a81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Define the loss function\n",
    "# ---------------------------------------------\n",
    "\n",
    "loss_func_name = 'square_loss'\n",
    "loss_func = define_loss_function(loss_func_name)\n",
    "\n",
    "\n",
    "if loss_func_name == 'square_loss':\n",
    "    gromov_loss_func = loss_func_name\n",
    "else:\n",
    "    gromov_loss_func = loss_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37e3272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of nodes. min:  100 max:  100 ave:  100.0\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Load the dataset\n",
    "# ---------------------------------------------\n",
    "\n",
    "def extract_TUDataset_graph(data: torch_geometric.data.data) -> Tuple[np.ndarray, csr_matrix, np.ndarray, Dict]:\n",
    "    \"\"\"\n",
    "    Extract the node distribution, adjacency matrix, feature matrix, and node index dictionary from the graph in TUDataset\n",
    "    Args:\n",
    "        graph: the graph instance generated via networkx\n",
    "    Returns:\n",
    "        probs: the distribution of nodes (n, )\n",
    "        adj: adjacency matrix (n, n)\n",
    "        x: nodes feauture matrix (n, num_feature)\n",
    "        idx2node: a dictionary {key: idx, value: node name}\n",
    "    \"\"\"\n",
    "    x, edge_index = data.x.numpy(), data.edge_index.numpy()\n",
    "    G = nx.Graph()\n",
    "    for ii in range(edge_index.shape[1]):\n",
    "        G.add_edge(edge_index[0,ii], edge_index[1,ii])\n",
    "    adj = nx.adjacency_matrix(G)\n",
    "    degrees = np.array(list(G.degree))[:, 1]\n",
    "    probs = degrees / np.sum(degrees)\n",
    "    \n",
    "    idx2node = {}\n",
    "    for i in range(len(probs)):\n",
    "        idx2node[i] = i\n",
    "    \n",
    "    return probs, csr_matrix(adj), x, idx2node\n",
    "\n",
    "\n",
    "dataset = TUDataset(root='/tmp/SYNTHETIC', name='SYNTHETIC', use_node_attr=True)\n",
    "num_graph = len(dataset)\n",
    "num_class = dataset.num_classes\n",
    "num_feature = dataset.num_node_features\n",
    "y_true = np.zeros(num_graph)\n",
    "num_node = []\n",
    "ii = 0\n",
    "for data in dataset:\n",
    "    y_true[ii] = data.y.numpy()\n",
    "    num_node.append(data.num_nodes)\n",
    "    ii += 1\n",
    "print('No. of nodes. min: ', min(num_node), 'max: ', max(num_node), 'ave: ', np.mean(num_node))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e3a3401",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Compute the distance matrix\n",
    "# ---------------------------------------------\n",
    "\n",
    "use_precompute_dist = True  # if 'True', use the precomputed FGW distance matrix approximated by Spar-GW\n",
    "\n",
    "if use_precompute_dist:\n",
    "    dist_mat = scipy.io.loadmat('results/SYNTHETIC_dist_mat_spargw.mat')['dist_mat_spargw']\n",
    "    \n",
    "else:\n",
    "    epsilon = 1e-2\n",
    "    alpha = 0.6\n",
    "    \n",
    "    dist_mat = np.zeros((num_graph, num_graph))\n",
    "    \n",
    "    for i in range(0, num_graph-1):\n",
    "        print('i: ', i)\n",
    "        \n",
    "        p_s, cost_s, x_s, idx2node_s = extract_TUDataset_graph(dataset[i])\n",
    "        C1 = cost_s.toarray().astype(float)\n",
    "    \n",
    "        for j in range(i+1, num_graph):\n",
    "            \n",
    "            p_t, cost_t, x_t, idx2node_t = extract_TUDataset_graph(dataset[j]) \n",
    "    \n",
    "            M = sp.spatial.distance.cdist(x_s, x_t)\n",
    "            M /= np.max(M)\n",
    "    \n",
    "            trans = spar_gw(cost_s, cost_t, p_s, p_t, gromov_loss_func, 2**5*max(M.shape), \n",
    "                            epsilon, M, alpha, random_state=123+100*j+421*i)\n",
    "            \n",
    "            dist_mat[i,j] = gw_distance(cost_s, cost_t, gromov_loss_func, trans, M, alpha)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba074bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering accuracy\n",
      "(ri) mean: 98.6711259754738 std: 0.0\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Graph clustering\n",
    "# ---------------------------------------------\n",
    "\n",
    "num_trial = 10\n",
    "ri_list = np.zeros((len(range(-10,11)), num_trial))\n",
    "ari_list = np.zeros((len(range(-10,11)), num_trial))\n",
    "\n",
    "for ii in range(-10,11):\n",
    "    gamma = 2**ii\n",
    "    kernel_mat = np.exp(-dist_mat/gamma)\n",
    "    \n",
    "    for jj in range(num_trial):\n",
    "        sc = SpectralClustering(n_clusters=num_class, \n",
    "                                random_state=1024+4321*jj,\n",
    "                                affinity='precomputed',\n",
    "                                assign_labels='discretize')\n",
    "        y_pred = sc.fit_predict(kernel_mat)\n",
    "    \n",
    "        rand_index = metrics.rand_score(y_true, y_pred)\n",
    "        ri_list[ii,jj] = rand_index\n",
    "\n",
    "\n",
    "mean_ri_list = np.nanmean(ri_list, axis=1)\n",
    "ri_max = np.max(mean_ri_list)\n",
    "ind_max = np.argmax(mean_ri_list)\n",
    "ri_std = np.std(ri_list[ind_max,])\n",
    "\n",
    "print('Clustering accuracy')\n",
    "print('(ri) mean:', ri_max*100, 'std:', ri_std*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0086989d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification accuracy\n",
      "mean: 98.79999999999998 std: 0.16329931618554355\n"
     ]
    }
   ],
   "source": [
    "#############################################################################\n",
    "#\n",
    "# Graph classification\n",
    "# ---------------------------------------------\n",
    "\n",
    "acc_list2 = []\n",
    "for cycle in range(10):\n",
    "    shuffle = np.random.choice(range(num_graph), size=num_graph, replace=False)\n",
    "    n_fold = 10\n",
    "    size_fold = int(num_graph/n_fold)\n",
    "    acc_list = np.zeros((len(range(-10,11)), n_fold))\n",
    "    \n",
    "    for ii in range(-10,11):\n",
    "        gamma = 2**ii\n",
    "        kernel_mat = np.exp(-dist_mat/gamma)\n",
    "        \n",
    "        for jj in range(n_fold):\n",
    "            test_ind = shuffle[(jj*size_fold):((jj+1)*size_fold)]\n",
    "            train_ind = np.delete(shuffle, range((jj*size_fold), ((jj+1)*size_fold)))\n",
    "            \n",
    "            kernel_mat_train = kernel_mat[train_ind,][:,train_ind]\n",
    "            kernel_mat_test = kernel_mat[test_ind,][:,train_ind]\n",
    "            y_train = y_true[train_ind]\n",
    "            y_test = y_true[test_ind]\n",
    "    \n",
    "            clf = svm.SVC(kernel=\"precomputed\")\n",
    "            clf.fit(kernel_mat_train, y_train)\n",
    "            y_pred = clf.predict(kernel_mat_test)\n",
    "            \n",
    "            acc_list[ii,jj] = np.sum(y_test==y_pred)/size_fold\n",
    "            \n",
    "\n",
    "    mean_acc_list = np.nanmean(acc_list, axis=1)\n",
    "    acc_max = np.max(mean_acc_list)\n",
    "    acc_list2 += [acc_max]\n",
    "    \n",
    "print('Classification accuracy')\n",
    "print('mean:', np.mean(acc_list2)*100, 'std:', np.std(acc_list2)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e113ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
